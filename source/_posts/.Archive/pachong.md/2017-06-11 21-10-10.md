##python使用BeautifulSoup爬取pixabay上的图片
我们最近也学了一些python的基础知识，难免会有点枯燥乏味，今天我们爬取`pixabay`上的图片并保存到本地。好，我们开始吧。
###先说一下我们使用的工具和python包
>python 3.6
>bs4 BeautifulSoup
>xlwt (用于保存到本地Excel)
>Chrome
> os

- 我使用的Python版本是3.6的，如果大家使用的是2版本，会稍微有点不同**想**
- 使用了** BeautifulSoup**简单来说，Beautiful Soup是python的一个库，最主要的功能是从网页抓取数据。官方解释如下：`Beautiful Soup提供一些简单的、python式的函数用来处理导航、搜索、修改分析树等功能。它是一个工具箱，通过解析文档为用户提供需要抓取的数据，因为简单，所以不需要多少代码就可以写出一个完整的应用程序。Beautiful Soup自动将输入文档转换为Unicode编码，输出文档转换为utf-8编码。你不需要考虑编码方式，除非文档没有指定一个编码方式，这时，Beautiful Soup就不能自动识别编码方式了。然后，你仅仅需要说明一下原始编码方式就可以了。Beautiful Soup已成为和lxml、html6lib一样出色的python解释器，为用户灵活地提供不同的解析策略或强劲的速度。`其实我们不用BeautifulSoup库也是可以的，不过是需要使用正则表达式截取网页信息，来得到我们需要的数据，`正则表达式：正则表通常被用来检索、替换那些符合某个模式(规则)的文本`
- 浏览器是使用的Chrome
- os 模块是Python标准库中的一个用于访问操作系统功能的模块

##所需要的知识点
- 列表
- for循环语句
- if 条件判断语句
- is关键字
- 字符串的拼接
##开始准备爬取
我们要爬取的网址是**https://pixabay.com/zh/photos/?q=%E9%A3%8E%E6%99%AF&image_type=&cat=&min_width=&min_height=**

![要爬取的页面](http://ophmqxrq8.bkt.clouddn.com/python2.png)
打开这个网址，按下F12，按照下图操作我们会看到我们需要爬取的网址

###首先我们先将整个网站的信息读取下来做分析`
```bash
#开始导入我们爬取过程中要使用的包
import urllib.request #这个大家还记得吗，我们在‘输入’课程中学过
import os
from bs4 import BeautifulSoup
#这个是要爬取的网站地址
url = 'https://pixabay.com/zh/photos/?q=%E9%A3%8E%E6%99%AF&image_type=&cat=&min_width=&min_height='
#先读取出我们要爬取的网址信息
res = urllib.request.urlopen(url)
html = res.read()
print(html)
```
我们会看到我们爬到的网址数据，哇太乱了怎么办？没关系难不住我们。
图1
我们还是在上次操作的网页上选中下图标识的位置，在网站上点击我们要爬取的图片，会出现这个图片的相关信息，如下图
图3
我们可以看到这个图片是在**img**标签下的，那么好了，我们先将所有**img**标签的数据爬取出来
```bash
import urllib.request
import os
from bs4 import BeautifulSoup
#这个是要爬取的网站地址
url = 'https://pixabay.com/zh/photos/?q=%E9%A3%8E%E6%99%AF&image_type=&cat=&min_width=&min_height='
#先读取出我们要爬取的网址信息
res = urllib.request.urlopen(url)
html = res.read()
#构建一个BeautifulSoup对象
soup = BeautifulSoup(html,'html.parser', from_encoding='utf-8')
#找出所以'img'标签对应的值
result = soup.find_all('img')
print(result)
```
执行完这段代码信息还是有点多，这样子，我们先摘取其中一段图片的信息
```bash
<img alt="草原, 公路, 风景, 山" src="https://cdn.pixabay.com/photo/2015/03/18/09/31/prairie-679016__340.jpg" srcset="https://cdn.pixabay.com/photo/2015/03/18/09/31/prairie-679016__340.jpg 1x, https://cdn.pixabay.com/photo/2015/03/18/09/31/prairie-679016__480.jpg 2x" title="草原, 公路, 风景, 山"/>
```
上面这段代码中，我们只需要得到`srcset="https://cdn.pixabay.com/photo/2015/03/18/09/31/prairie-679016__340.jpg 1x, https://cdn.pixabay.com/photo/2015/03/18/09/31/prairie-679016__480.jpg 2x"`就可以了，然后我们将这段代码的图片的路径截取出来也就是（**https://cdn.pixabay.com/photo/2015/03/18/09/31/prairie-679016__340.jpg**），这里我们会使用到字符串的截取操作，来看一下代码
```bash
#我们循环 将得到的 数据截取出我们需要的图片路径
for content in result:
    #担心下载太多（毕竟是测试）做了一个数量限制（我们做测试不需要爬取太多图片）
	if index < 100:
    	#这里我们将每段img标签下内容中的srcset标签所对应的数据拿出来
		s = content.get('srcset')
		if s is None:
			ss = s
		else:
			links.append(s.split(' ')[0]) #append()方法是忘列表中添加元素，split()是字符串的拆分操作，会返还一个拆分后的字符串列表
			#例如 s = 'asd'    s.split('s')   会得到 一个列表 ['a','d']
	index+=1
```
到这里我们就能拿到所有图片的地址了，可以将其打印出来，这里先列出来一部分
```bash
['https://cdn.pixabay.com/photo/2015/03/18/09/31/prairie-679016__340.jpg',
'https://cdn.pixabay.com/photo/2015/03/18/09/31/prairie-679014__340.jpg','
https://cdn.pixabay.com/photo/2015/03/18/09/29/the-scenery-679011__340.jpg',
 'https://cdn.pixabay.com/photo/2014/07/30/02/00/iceberg-404966__340.jpg',
 #...这里就不一一列出来了
```
我们拿到了图片的地址下面我们就可以下载图片了
```bash
if not os.path.exists('photo'):
	os.makedirs('photo')
#循环把图片下载到本地photo路径下
i = 0
#for循环 循环读取我们爬取到的 图片地址列表
for link in links:
	i+=1
	filename = 'photo\\'+'photo'+str(i)+'.jpg'
	with open(filename,'w'):
		urllib.request.urlretrieve(link,filename)
```
``` bash
#开始导入我们爬取过程中要使用的包
import urllib.request
import os
from bs4 import BeautifulSoup
#这个是要爬取的网站地址
url = 'https://pixabay.com/zh/photos/?q=%E9%A3%8E%E6%99%AF&image_type=&cat=&min_width=&min_height='
#先读取出我们要爬取的网址信息
res = urllib.request.urlopen(url)
html = res.read()
#构建一个BeautifulSoup对象
soup = BeautifulSoup(html,'html.parser', from_encoding='utf-8')
#找出所以'img'标签对应的值
result = soup.find_all('img')
links = []
index = 0
for content in result:
    #担心下载太多（毕竟是测试）做了一个数量限制（我们做测试不需要爬取太多图片）
	if index < 100:
		s = content.get('srcset')
		if s is None:
			ss = s
		else:
			links.append(s.split(' ')[0])
	index+=1
	
#输出一共找出几个符合的图片地址
print(len(links))
#判断本地是否有photo这个路径，没有的话创建一个
if not os.path.exists('photo'):
	os.makedirs('photo')

#循环把图片下载到本地photo路径下
i = 0
#for循环 循环读取我们爬取到的 图片地址列表
for link in links:
	i+=1
	filename = 'photo\\'+'photo'+str(i)+'.jpg'
	with open(filename,'w'):
		urllib.request.urlretrieve(link,filename)

```

## 爬取结果
![保存到本地的爬取结果](http://ophmqxrq8.bkt.clouddn.com/python1.png)